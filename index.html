<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Object Recognition Project</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Object Recognition Project</h1>
            <nav>
                <ul>
                    <li><a href="#problem">Problem</a></li>
                    <li><a href="#dataset">Dataset</a></li>
                    <li><a href="#models">Models</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#team">Team</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <section id="problem">
        <div class="container">
            <h2>Problem in Detail</h2>
            <p>Object recognition, the ability to identify objects in natural scenes, holds significant importance across various fields due to its wide-ranging applications. In this project, our aim was to evaluate and compare the efficacy of machine learning (ML) techniques for the task of object recognition. Leveraging the CIFAR-10 dataset, which comprises images across ten different classes, we implemented and analyzed several ML models. Our study focused on comparing the performance of five prominent models: K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Neural Network (NN), Decision Tree, Random Forest, and Ensemble Learning. Each model was trained and tested on the CIFAR-10 dataset to assess its accuracy and effectiveness in recognizing objects within natural scenes.</p>
        </div>
    </section>

    <section id="dataset">
        <div class="container">
            <h2>Info about the Dataset</h2>
            <img src="dataset.png" alt="CIFAR-10 Dataset">
            <p>The CIFAR-10 dataset is a well-known benchmark dataset in the field of computer vision and machine learning. It consists of 60,000 32x32 color images in 10 different classes, with each class representing a specific object or category. The dataset is divided into 50,000 training images and 10,000 test images, making it suitable for training and evaluating machine learning models for object recognition tasks.</p>
            <p>The 10 classes in the CIFAR-10 dataset include common objects such as airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each class contains an equal number of images, resulting in a balanced dataset. The images are low-resolution, which adds an additional challenge to the task of object recognition.</p>
            <p>The CIFAR-10 dataset is widely used for research and benchmarking purposes due to its variety of object classes, large number of images, and relatively small image size, making it computationally tractable for experimentation. Many state-of-the-art machine learning algorithms and deep learning architectures have been trained and evaluated on this dataset, making it a standard benchmark for evaluating the performance of new models and techniques in the field of computer vision.</p>
    </div>
        </div>
    </section>

    <section id="results">
        <div class="container">
            <h2>Results and Accuracy</h2>
            <h3>Technique 1</h3>
            <table align="center">
                <tr>
                    <th>Model</th>
                    <th>Accuracy (%)</th>
                </tr>
                <tr>
                    <td>Support Vector Machine (SVM)</td>
                    <td>61</td>
                </tr>
                <tr>
                    <td>Logistic Regression</td>
                    <td>48</td>
                </tr>
                <tr>
                    <td>Decision Tree</td>
                    <td>27</td>
                </tr>
                <tr>
                    <td>Decision Tree with best hyperparameters</td>
                    <td>30</td>
                </tr>
                <tr>
                    <td>Random Forest</td>
                    <td>50</td>
                </tr>
                <tr>
                    <td>K-Nearest Neighbors (KNN)</td>
                    <td>53</td>
                </tr>
                <tr>
                    <td>Ensemble Learning</td>
                    <td>53</td>
                </tr>
                <tr>
                    <td>Multi-Layer Perceptron (MLP)</td>
                    <td>56</td>
                </tr>
            </table>
            <h3>Technique 2</h3>
            <table align="center">
                <tr>
                    <th>Model</th>
                    <th>Accuracy (%)</th>
                </tr>
                <tr>
                    <td>K-Nearest Neighbors (KNN)</td>
                    <td>38</td>
                </tr>
                <tr>
                    <td>Logistic Regression</td>
                    <td>40</td>
                </tr>
                <tr>
                    <td>Random Forest</td>
                    <td>43</td>
                </tr>
                <tr>
                    <td>Decision Tree</td>
                    <td>27</td>
                </tr>
                <tr>
                    <td>Decision Tree with best hyperparameters</td>
                    <td>30</td>
                </tr>
            </table>
            <p>In the first technique, we employed a preprocessing pipeline consisting of grayscale conversion followed by Histogram of Oriented Gradients (HOG) feature extraction. The classification was then performed using Support Vector Machine (SVM), which achieved the highest accuracy of 61%. However, other classifiers, such as MLP, ADAboost, random forest, logistic regression, and ensemble learning, also yielded competitive accuracies ranging from 40% to 56%.</p> 
            <p>For the second technique, we applied PCA followed by Linear Discriminant Analysis (LDA) for dimensionality reduction. Despite the different preprocessing steps, the classification accuracies varied across classifiers. While logistic regression and random forest achieved accuracies of 40% and 43%, respectively, KNN and decision tree classifiers showed lower accuracies of 38% and 27%, respectively. However, with hyperparameter tuning, the decision treeâ€™s accuracy improved to 30%. Comparing the two techniques, we observed that the first technique, utilizing HOG feature extraction and SVM classification, generally outperformed the second technique, which employed PCA and LDA. However, the decision tree classifier with optimized hyperparameters from the second technique exhibited a comparable accuracy to several classifiers from the first technique. 
            <p>Overall, the choice of preprocessing pipeline and classifier significantly influenced the classification performance, with the first technique demonstrating better overall accuracy across various classifiers.</p>
        </div>
    </section>
    
    
    
    

    

    <section id="team">
        <div class="container">
            <h2>Team Members</h2>
            <ul class="team-list">
                <li>Shahil Sharma (B22CS048)</li>
                <li>Shivanshu Verma(B22ES010)</li>
                <li>Om Prakash Nain(B22AI062)</li>
                <li>Amol Gaur(B22CS008)</li>
                <li>Tanisha Mangliya(B22EE067)</li>
                <li>Abhishek Yadav(B22ES020)</li>
                <li>Binay Suman(B22CS019)</li>
            </ul>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2024 Object Recognition Project. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
